{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eec02c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service as ChromiumService\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df6d5a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"accept\": \"*/*\",\n",
    "    \"accept-encoding\": \"gzip, deflate, br, zstd\",\n",
    "    \"accept-language\": \"en-US,en;q=0.6\",\n",
    "    \"cache-control\": \"no-cache\",\n",
    "    \"content-type\": \"application/x-protobuf\",\n",
    "    \"cookie\": \"_GRECAPTCHA=09AMNxLB8XaXxNkOyNW3EHzAfcM_Xw2f03cxPCxNgyj0fitf-42B0ttj0FevJvOQ5MTQz-HK9O_0SFadw2i9JYoBE\",\n",
    "    \"origin\": \"https://www.google.com\",\n",
    "    \"pragma\": \"no-cache\",\n",
    "    \"sec-ch-ua\": \"\\\"Chromium\\\";v=\\\"136\\\", \\\"Brave\\\";v=\\\"136\\\", \\\"Not.A/Brand\\\";v=\\\"99\\\"\",\n",
    "    \"sec-ch-ua-mobile\": \"?0\",\n",
    "    \"sec-ch-ua-platform\": \"\\\"Windows\\\"\",\n",
    "    \"sec-fetch-dest\": \"empty\",\n",
    "    \"sec-fetch-mode\": \"cors\",\n",
    "    \"sec-fetch-site\": \"same-origin\",\n",
    "    \"sec-fetch-storage-access\": \"none\",\n",
    "    \"sec-gpc\": \"1\",\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ec80c8",
   "metadata": {},
   "source": [
    "<!-- Jio -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ef15a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✔] Scraped: https://www.cardekho.com\n",
      "[✔] Scraped: https://www.cardekho.com/newcars\n",
      "[✔] Scraped: https://www.cardekho.com/mg/windsor-ev\n",
      "[✔] Scraped: https://www.cardekho.com/mahindra/be-6\n",
      "[✔] Scraped: https://www.cardekho.com/mahindra/xev-9e\n",
      "[✔] Scraped: https://www.cardekho.com/mg/comet-ev\n",
      "[✔] Scraped: https://www.cardekho.com/tata/curvv-ev\n",
      "[✔] Scraped: https://www.cardekho.com/tata/nexon-ev\n",
      "[✔] Scraped: https://www.cardekho.com/electric-cars\n",
      "[✔] Scraped: https://www.cardekho.com/mahindra/scorpio-n\n",
      "[✔] Scraped: https://www.cardekho.com/mahindra/thar\n",
      "[✔] Scraped: https://www.cardekho.com/mahindra/xuv700\n",
      "[✔] Scraped: https://www.cardekho.com/hyundai/creta\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 97\u001b[0m\n\u001b[0;32m     95\u001b[0m base_domain \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcardekho.com\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     96\u001b[0m output_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCarDekho\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 97\u001b[0m \u001b[43mscrape_sites\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_domain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 50\u001b[0m, in \u001b[0;36mscrape_sites\u001b[1;34m(base_url, base_domain, output_folder)\u001b[0m\n\u001b[0;32m     47\u001b[0m visited_urls\u001b[38;5;241m.\u001b[39madd(normalized_url)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 50\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_delay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(normalized_url, headers\u001b[38;5;241m=\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "def is_valid_url(url, base_domain):\n",
    "    \"\"\"\n",
    "    Check if the URL is valid and belongs to the base domain (including subdomains).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parsed = urlparse(url)\n",
    "        return (\n",
    "            parsed.scheme in ['http', 'https'] and\n",
    "            parsed.netloc.endswith(base_domain)\n",
    "        )\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def clean_text(row_text: str) -> str:\n",
    "    # Remove newlines and extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', row_text)  # Replace all whitespace (tabs, newlines, etc.) with a single space\n",
    "    text = text.strip()                   # Remove leading and trailing whitespace\n",
    "    text = text.lower()                   # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "def scrape_sites(base_url, base_domain, output_folder):\n",
    "    # --- Core Configuration ---\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
    "    # --- Setup ---\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    visited_urls = set()\n",
    "    urls_to_visit = [base_url]\n",
    "    request_delay = 1  # seconds\n",
    "\n",
    "    while urls_to_visit:\n",
    "        current_url = urls_to_visit.pop(0)\n",
    "\n",
    "        # Normalize current URL\n",
    "        normalized_url = current_url.split('#')[0].rstrip('/')\n",
    "\n",
    "        if normalized_url in visited_urls:\n",
    "            continue\n",
    "\n",
    "        visited_urls.add(normalized_url)\n",
    "\n",
    "        try:\n",
    "            time.sleep(request_delay)\n",
    "            response = requests.get(normalized_url, headers=headers, timeout=10)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "                # --- Extract & Save Page Content ---\n",
    "                page_text = clean_text(soup.get_text())\n",
    "                page_title = soup.title.string.strip() if soup.title else \"NoTitle\"\n",
    "                safe_filename = \"\".join(c if c.isalnum() else \"_\" for c in page_title)[:100]\n",
    "                file_name = f\"{safe_filename}.txt\"\n",
    "                file_full_path = os.path.join(output_folder, file_name)\n",
    "\n",
    "                with open(file_full_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(page_text)\n",
    "\n",
    "                 # Store file logs\n",
    "                with open(\"logs.txt\", \"a\", encoding=\"utf-8\") as f: f.write(f\"{current_url}, {file_name}\\n\")\n",
    "\n",
    "                print(f\"[✔] Scraped: {normalized_url}\")\n",
    "\n",
    "                # --- Find and Queue Internal Links ---\n",
    "                for a_tag in soup.find_all('a', href=True):\n",
    "                    href = a_tag['href'].strip()\n",
    "\n",
    "                    # Ignore invalid links\n",
    "                    if href.startswith(('mailto:', 'tel:', 'javascript:')):\n",
    "                        continue\n",
    "\n",
    "                    full_url = urljoin(normalized_url, href)\n",
    "                    clean_url = full_url.split('#')[0].rstrip('/')\n",
    "\n",
    "                    if is_valid_url(clean_url, base_domain) and clean_url not in visited_urls:\n",
    "                        urls_to_visit.append(clean_url)\n",
    "\n",
    "            else:\n",
    "                print(f\"[✖] Failed (Status {response.status_code}): {normalized_url}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[!] Error scraping {normalized_url}: {str(e)[:100]}\")\n",
    "\n",
    "    print(f\"\\n✅ Scraping completed. Total pages scraped: {len(visited_urls)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_url = \"https://www.cardekho.com/\"\n",
    "    base_domain = \"cardekho.com\"\n",
    "    output_folder = \"CarDekho\"\n",
    "    scrape_sites(base_url, base_domain, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ab415d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
